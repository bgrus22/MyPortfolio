{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b04714-5926-40bd-9317-e93fd88b93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required libraries \n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7273a50-7a4e-4f7a-bb39-a6e80e291bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp==4.2.1\n",
      "  Using cached spark_nlp-4.2.1-py2.py3-none-any.whl (643 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "  Attempting uninstall: spark-nlp\n",
      "    Found existing installation: spark-nlp 4.2.1\n",
      "    Uninstalling spark-nlp-4.2.1:\n",
      "      Successfully uninstalled spark-nlp-4.2.1\n",
      "Successfully installed spark-nlp-4.2.1\n",
      "Requirement already satisfied: sparknlp in /mnt/miniconda/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: spark-nlp in /mnt/miniconda/lib/python3.7/site-packages (from sparknlp) (4.2.1)\n",
      "Requirement already satisfied: numpy in /mnt/miniconda/lib/python3.7/site-packages (from sparknlp) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!/mnt/miniconda/bin/pip install spark-nlp==4.2.1 --force\n",
    "!/mnt/miniconda/bin/pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eec03f-0422-42be-a096-76064043605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f142f47d-8e24-491d-98f3-c03b08b3b0af;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 447ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f142f47d-8e24-491d-98f3-c03b08b3b0af\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/12ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/03 20:41:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.2.1.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.typesafe_config-1.4.2.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.navigamez_greex-1.0.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.3.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/com.google.code.gson_gson-2.3.jar added multiple times to distributed cache.\n",
      "23/04/03 20:41:33 WARN Client: Same path resource file:///home/hadoop/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SparkNLP\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.1\") \\\n",
    "    .master('yarn') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaba3d7-fdee-4d25-ba16-c673feca7426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-14-208.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkNLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f092eab5b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3466ec-5b93-4bfb-90ea-5aa23bd5ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing spark nlp libraries\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ada760-f76b-4ef5-8fa7-0110036b7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Importing the data \n",
    "\n",
    "df_com = spark.read.parquet(\"s3a://ppol567-llj40-bucket-3/cleaned_comments\")\n",
    "df_sub = spark.read.parquet(\"s3a://ppol567-llj40-bucket-3/cleaned_submissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1214d7b-4fb3-4b22-9411-2fb30e446991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16171595\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: double (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- body_length: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- date_clean: date (nullable = true)\n",
      " |-- submission_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146327\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- edited: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title_length: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- date_clean: date (nullable = true)\n",
      " |-- Top_100: integer (nullable = true)\n",
      " |-- Live_Thread: boolean (nullable = true)\n",
      " |-- War_Dummy: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Viewing the specs\n",
    "print(df_com.count())\n",
    "df_com.printSchema()\n",
    "\n",
    "print(df_sub.count())\n",
    "df_sub.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc95a73-59e2-48c0-9c3e-f66c32988bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test Dataframe to run the preprocessing pipeline\n",
    "df_title = df_sub.select(\"id\", \"title\")\n",
    "df_title.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79927231-9a26-4e07-a48b-42e26dff79c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[ | ]stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "### Cleaning the text \n",
    "\n",
    "#### Intializing document assembler \n",
    "documentAssembler = DocumentAssembler().setInputCol(\"body\").setOutputCol(\"document\")\n",
    "\n",
    "### Tokenizing the document \n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(\"document\") \\\n",
    "            .setOutputCol(\"token\")\n",
    "\n",
    "#### Removing English stop-words\n",
    "stop_words = StopWordsCleaner.pretrained(\"stopwords_en\", \"en\") \\\n",
    "                             .setInputCols([\"token\"]) \\\n",
    "                             .setOutputCol(\"sw_rem\")\n",
    "\n",
    "\n",
    "\n",
    "### Cleaning data to remove special characters and non-english words and converting to lower case\n",
    "cleanUpPatterns = [\"[^A-Za-z0-9 ]\"]\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols([\"sw_rem\"]) \\\n",
    "     .setOutputCol(\"normalized\") \\\n",
    "     .setLowercase(True) \\\n",
    "     .setCleanupPatterns(cleanUpPatterns) \n",
    "\n",
    "#### Applying Lemmatization\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols([\"normalized\"]) \\\n",
    "     .setOutputCol(\"clean\")\n",
    "\n",
    "\n",
    "### Transforming into human-readable form using finisher\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['clean']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2aa56f7-f4a1-482e-8998-d04beadff246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Preprocessing pipeline\n",
    "pre_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler, \n",
    "          tokenizer,\n",
    "          stop_words,\n",
    "          normalizer, \n",
    "          lemmatizer, \n",
    "          finisher\n",
    "      ])\n",
    "### Testing the pipeline on our test dataframe \n",
    "preprocessed_df = pre_pipeline.fit(df_title).transform(df_title)\n",
    "preprocessed_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155aba35-2a59-48ed-9049-2f655cff42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting required columns from the datasets \n",
    "df_sub_req = df_sub.select([\"id\", \"title\", \"created_date\", \"date_clean\", \"Live_Thread\", \"War_Dummy\"])\n",
    "df_com_req = df_com.select([\"id\", \"submission_id\", \"created_date\", \"date_clean\", \"body\", \"controversiality\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3305fd65-fd04-4552-8c2f-0b7fef56b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Preprocessing the dataframes above \n",
    "df_sub_preprocessed = pre_pipeline.fit(df_sub_req).transform(df_sub_req).write.parquet(\"s3a://ppol567-llj40-bucket-3/preprocessed_submissions\")\n",
    "df_com_preprocessed = pre_pipeline.fit(df_com_req).transform(df_com_req).write.parquet(\"s3a://ppol567-llj40-bucket-3/preprocessed_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Length Graphs \n",
    "bin_width_sub = 10\n",
    "bin_width_com = 5\n",
    "\n",
    "#Counting the number of words in each submission and comment \n",
    "df_sub = df_sub.withColumn('no_of_words', f.size(f.split(col('title'), ' ')))\n",
    "df_com = df_com.withColumn('no_of_words', f.size(f.split(col('body'), ' ')))\n",
    "\n",
    "### Binning the values \n",
    "df_sub_words = df_sub.withColumn(\"bucket\", (col(\"no_of_words\")/bin_width_sub).cast('int')).select(\"no_of_words\", \"bucket\")\n",
    "df_com_words = df_com.withColumn(\"bucket\", (col(\"no_of_words\")/bin_width_com).cast('int')).select(\"no_of_words\", \"bucket\")\n",
    "\n",
    "### Generating the tables \n",
    "df_sub_words = df_sub_words.groupby(\"bucket\") \\\n",
    "                           .count().orderBy(\"bucket\").toPandas()\n",
    "\n",
    "df_com_words = df_com_words.groupby(\"bucket\") \\\n",
    "                           .count().orderBy(\"bucket\").toPandas()\n",
    "\n",
    "### Saving as csv files \n",
    "df_sub_words.to_csv(\"Word_Distribution_Submissions.csv\")\n",
    "df_com_words.to_csv(\"Word_Distribution_Comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a37005",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most Common Words \n",
    "### Training a count vectorizer \n",
    "cv = CountVectorizer(inputCol = \"finished_clean\", outputCol = \"features\", minDF = 1.0)\n",
    "cv_model = cv.fit(df_sub_preprocessed)\n",
    "result = cv_model.transform(df_sub_preprocessed)\n",
    "\n",
    "cv_result = result.withColumn(\"features\", col(\"features\").cast('string')).select(\"finished_clean\", \"features\")\n",
    "\n",
    "#Function definition to get first parameter\n",
    "def first_params(row):\n",
    "    match1 = re.findall(r\"\\[(.*?)\\]\", row)\n",
    "    return match1[0]\n",
    "\n",
    "#Function definition to get second parameter\n",
    "def second_params(row):\n",
    "    match2 = re.findall(r\"\\[(.*?)\\]\", row)\n",
    "    return match2[1]\n",
    "\n",
    "#Registering as udf\n",
    "udf_1 = f.udf(lambda z: first_params(z))\n",
    "udf_2 = f.udf(lambda z: second_params(z))\n",
    "\n",
    "### Extracting features through udfs \n",
    "cv_result_final = cv_result.withColumn(\"param1\", udf_1(f.col(\"features\"))).withColumn(\"param2\", udf_2(f.col(\"features\")))\n",
    "\n",
    "### Converting in a form to enable explode \n",
    "cv_result_final = cv_result_final.withColumn(\"features\", f.split(f.col(\"features\"),\",\")) \\\n",
    "                                 .withColumn(\"param1\", f.split(f.col(\"param1\"), \",\")) \\\n",
    "                                 .withColumn(\"param2\", f.split(f.col(\"param2\"), \",\"))\n",
    "\n",
    "### Implementing the explode \n",
    "final_cv = cv_result_final.select(\"features\", f.explode(f.arrays_zip(f.col(\"finished_clean\"), f.col(\"param1\"), f.col(\"param2\"))).alias(\"Params\"))\\\n",
    "                          .select(\"features\",f.col(\"Params.finished_clean\").alias(\"word\"), f.col(\"Params.param1\").alias(\"word_index\"), f.col(\"Params.param2\").alias(\"frequency\"))\n",
    "\n",
    "### Viewing the results \n",
    "final_cv = final_cv.select('word', 'word_index', 'frequency')\n",
    "\n",
    "final_cv.show(3)\n",
    "\n",
    "### Filtering the top 15 words \n",
    "top_words_df = final_cv.groupby('word').agg(f.count('word').alias('Total Occurence')).orderBy(f.col('Total Occurence').desc()).limit(15).toPandas()\n",
    "\n",
    "### Saving as csv file\n",
    "top_words_df.to_csv('top_words_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most Important Words \n",
    "### Obtaining TF-IDF \n",
    "h_t = HashingTF(inputCol = \"finished_clean\", outputCol = \"features\")\n",
    "tf = h_t.transform(df_sub_preprocessed)\n",
    "\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"TF_IDF_features\", minDocFreq=10)\n",
    "idf_model = idf.fit(tf)\n",
    "tfidf = idf_model.transform(tf)\n",
    "\n",
    "tfidf.select(\"finished_clean\", \"TF_IDF_features\").show(5, truncate=False)\n",
    "\n",
    "tfidf_result = tfidf.withColumn(\"TF_IDF_features\", col(\"TF_IDF_features\").cast('string')).select(\"finished_clean\", \"TF_IDF_features\")\n",
    "\n",
    "### Extracting features through udfs \n",
    "tfidf_result_final = tfidf_result.withColumn(\"param1\", udf_1(f.col(\"TF_IDF_features\"))).withColumn(\"param2\", udf_2(f.col(\"TF_IDF_features\")))\n",
    "### Converting in a form to enable explode\n",
    "tfidf_result_final = tfidf_result_final.withColumn(\"TF_IDF_features\", f.split(f.col(\"TF_IDF_features\"),\",\")) \\\n",
    "                                       .withColumn(\"param1\", f.split(f.col(\"param1\"), \",\")) \\\n",
    "                                       .withColumn(\"param2\", f.split(f.col(\"param2\"), \",\"))\n",
    "\n",
    "### Implementing the explode \n",
    "final_tfidf = tfidf_result_final.select(\"TF_IDF_features\", f.explode(f.arrays_zip(f.col(\"finished_clean\"), f.col(\"param1\"), f.col(\"param2\"))).alias(\"Params\")) \\\n",
    "                                .select(\"TF_IDF_features\",f.col(\"Params.finished_clean\").alias(\"word\"), f.col(\"Params.param1\").alias(\"word_index\"), f.col(\"Params.param2\").alias(\"TF_IDF_Score\"))\n",
    "\n",
    "### Viewing the results                              \n",
    "final_tfidf.show(5)\n",
    "\n",
    "final_tfidf = final_tfidf.withColumn(\"TF_IDF_Score\", col(\"TF_IDF_Score\").cast(FloatType()))\n",
    "\n",
    "### Filtering the top 12 words\n",
    "tf_idf_df = final_tfidf.orderBy(f.col('TF_IDF_Score').desc()).select('word', 'TF_IDF_Score').limit(12).toPandas()\n",
    "### Saving as csv file\n",
    "tf_idf_df.to_csv('tf_idf_sub.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1936d817-2084-4d9d-8427-654969f928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
